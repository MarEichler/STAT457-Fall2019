---
title: "STAT 457 - FINAL"
author: "Martha Eichlersmith"
date: "2019-12-12"
output: 
  pdf_document:
    fig_caption: yes
#    number_sections: true
header-includes: 
- \usepackage{color}
- \usepackage{mathtools}
- \usepackage{bbm} #for mathbb for numbers
- \usepackage{amsbsy}
- \usepackage{caption} #to remove automatic table name and number - \captionsetup[table]{labelformat=empty}, put code under YAML
- \usepackage{booktabs}
- \usepackage{geometry}
- \usepackage{float} #to hold things in place
- \floatplacement{figure}{H}
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \fancyhead[L]{STAT 457 Fall 2019 \\ Final}
- \fancyhead[R]{Martha Eichlersmith \\ Page \thepage\ of\ \pageref*{LastPage}}  
- \setlength{\headheight}{22.5pt} #to remove \fancyhead error for head height
geometry: "left=0.75in,right=0.75in,top=1.1in,bottom=1in" 
---

\captionsetup[table]{labelformat=empty}
```{r setup, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
library(ggplot2) #ggplot
library(gridExtra) #organize plots
library(grid) #organize plots
library(latex2exp) #latex in ggplot titles 
library(dplyr) #for piping 
knitr::opts_chunk$set(fig.width = 10, fig.height = 4)
knitr::opts_chunk$set(echo=FALSE)
decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
dec <- 5
#knitr::opts_chunk$set(echo=FALSE) 
```

\newpage  
# Problem 1  
## Problem 1a  
For the data $Y = (125,18,20,34)$, implement the Gibbs sampler algorithm.  Use a flat prior on $\theta$.  
Plot $\theta^i$ versus iteration $i$. 
$Y = (y_1, y_2, y_3, y_4) \propto (2 + \theta, 1 - \theta, 1 - \theta, \theta)$  
1. Draw a starting value, $t \sim$Uniform(0,1)  
2. Draw a latent value, $Z \sim \text{Binomial}\left( y_1, \ \frac{ \theta}{2 + \theta} \right)$  
3. Draw a parameter, $\theta \sim \text{Beta}( Z+y_4 + 1, y_2 + y_3 + 1)$  

```{r p1.func_chain}
func_chain <-function(startseed, Y){
  set.seed(startseed)
  t <- runif(1)
  theta <- t
  chain<- c()#rep(NA, 10000)
  chain[1]<-theta
  Z.i<-rbinom(1,Y[1],(theta/(theta+2)))
  theta.i<-rbeta(1,Z.i+Y[4]+1,Y[2]+Y[3]+1)
  chain[2]<-theta.i
  Z.i<-rbinom(1,Y[1],(theta.i/(theta.i+2)))
  theta.i<-rbeta(1,Z.i+Y[1]+1,Y[2]+Y[3]+1)
  chain[3]<-theta.i
  k<-3
    while(abs(chain[k]-chain[k-1]) >= 0.000001) {
      Z.i<-rbinom(1,Y[1], (theta.i/(theta.i+2)))
      theta.i<-rbeta(1,Z.i+Y[4]+1,Y[2]+Y[3]+1)
      k<-k+1
      chain[k]<-theta.i
    }
  chain<-chain[!is.na(chain)]
  chain
}
```

```{r p1.scale_funcs}
func_scalelike<-function(x,y1,y2,y3,y4){
  like <-(2+x)^y1*(1-x)^(y2+y3)*(x)^y4
  like.max<-max(like)
  like/like.max #normalized likelihood (on scale from 0 to 1) 
}

func_scalenormal<-function(x, mean, sd){
  scales::rescale(dnorm(x, mean, sd), to=c(0, 1)) #normal (on scale from 0 to 1) 
}
```

```{r p1.func_p1AB}
func_problem1AB <- function(startseed, Y, number){

chain <- func_chain(startseed, Y)
  
df.chain <- data.frame(
        "theta.i" = chain,
        "i"=c(1:length(chain))
)

chain.mu <- mean(chain)
chain.sd <- sd(chain)
table <- data.frame(
  "Name" = c("Mean", "SD", "it", "Start"),
  "Value" = c(
    decimal(chain.mu, dec), 
    decimal(chain.sd, dec), 
    decimal(length(chain), 0),
    decimal(chain[1], dec)
  )
)
tg <- tableGrob(table)

print.Y <- paste(Y, collapse = ",")

#plot theta_i versus i (iterations)
plot.Gibbs <- ggplot(df.chain, aes(x=i, y=theta.i))+
  geom_line(alpha=0.4)+
  ggtitle("Gibbs Sampler")


colors <- c("navy", "maroon")
norm.like <-stat_function(
  fun = func_scalelike
  , args = list(y1=Y[1], y2=Y[2], y3=Y[3], y4=Y[4])
  , lwd = 1
  , linetype="solid"
  , aes(col="Normalized Like.")
  )
normal.approx <-stat_function(
  fun = func_scalenormal
  , args = list(mean=chain.mu, sd=chain.sd)
  , lwd = 1.5
  , linetype="dotted"
  , aes(col="Normal Approx."))

name <- paste("Normal Likelihood and Normal Approx")

#print normalized likelihoods 
x <- seq(0, 1, 0.001)
df.x <- data.frame("X"=x)
plot.Like <-  ggplot(data=df.x, aes(x=X))+
  norm.like+normal.approx+
  ggtitle(paste(name))+
  theme(  axis.title.x = element_blank()
         ,axis.title.y = element_blank()
         ,axis.text.x = element_blank()
         ,axis.text.y = element_blank()
        )+
  scale_colour_manual("", values = c(colors[1], colors[2])) +
  theme(legend.position = c(.2,.9))

main <- paste("Chain", number, "for data for Y=(", print.Y, ")")

gs <- list(plot.Gibbs, tg, plot.Like)
grid.arrange(grobs=gs, nrow=1, widths=c(2, 1, 2),
             top = textGrob(main, vjust = .5, gp = gpar(fontface = "bold", cex = 1.2))
            )
}
```

```{r p1aRESULT, fig.height=2}
Y.A <- c(125, 18, 20, 34)

func_problem1AB(111, Y.A, 1)
func_problem1AB(112, Y.A, 2)
func_problem1AB(113, Y.A, 3)
func_problem1AB(114, Y.A, 4)
func_problem1AB(115, Y.A, 5)
```

\newpage 
## Problem 1b  
Repeat 1a for $Y = (14, 0, 1, 5)$.  
```{r p1bRESULT, fig.height=2}
Y.B <- c(14,0,1,5)

func_problem1AB(121, Y.B, 1)
func_problem1AB(122, Y.B, 2)
func_problem1AB(123, Y.B, 3)
func_problem1AB(124, Y.B, 4)
func_problem1AB(125, Y.B, 5)
```
There is a lack of fit for the data in 1b, where the fit appears to be better for data in 1a. Convergence was assessed when values were had a difference less than $10^{-7}$.   

\newpage
## Problem 1c   
```{r p1.func_chaininfo}
func_chaininfo <- function(startseed, Y){
chain <- func_chain(startseed, Y)
it <- length(chain)
chain.mu <- mean(chain)
chain.sd <- sd(chain)
chain.se <- chain.sd/sqrt(it)
vec <-c(chain.mu, chain.sd, chain.se, it)
return(vec)
}
```

```{r p1cRESULT20chains}
Y.C <- c(125, 18, 20, 34)
print.Y.C <- paste(Y.C, collapse=",")
cnames <- c("Mean", "Standard Deviation", "Standard Error", "Iterations")
info.20chain <- mapply(func_chaininfo, startseed=c(1:20), Y=rep(list(Y.C), 20))
info.20chain <- t(info.20chain)
colnames(info.20chain) <- cnames

knitr::kable(info.20chain, align='rrrr', digits=dec, caption=paste("20 Chains for Y=(", print.Y.C, ")"))
```

```{r p1cRESULTavgsd.vs.se}
sd.of.avgs <- sd(info.20chain[,1])
avg.of.se <- mean(info.20chain[,3])

paste(round(sd.of.avgs, dec+1), "is the standard deviation of the 20 averages of theta")
paste(decimal(avg.of.se, dec+1), "is the average of the standard errors of the 20 chains of theta")
```

You would expected these values to be similar but it appears that our standard error average is under-estimating the variation in the chain means of $\theta$ in this case.  


\newpage  
# Problem 2  

## Problem 2a  
For the genetic linkage model applied to $Y = (125, 18, 20, 34)$, implement the Metropolis algorithm.  
(use a flat prior on $\theta)$.  Use one long chain and plot $\theta^i$ versus $i$.  
Try several driver functions: 

```{r p2.func_pi,func_Metropolis,func_drivers}
func_pi <- function(theta, Y){
  (2+theta)^(Y[1]) * (1 - theta)^(Y[2]+Y[3]) * (theta)^(Y[4])
}

m <- 10

#theta.i = X (old value)
#theta.j = Y (new value) 
theta.i <- 0.1
Y <- c(125, 18, 20, 34)

func_MetroFix <- function(startseed, m, Y, driver){
  #fixed driver 
  set.seed(startseed)
  theta.i <- runif(1)
  chain <- c()
  alpha <- c()
  chain[1] <- theta.i
  for (k in 2:m){
    theta.j <- driver(1)
    alpha.ij <- min(c(1, func_pi(theta.j, Y)/func_pi(theta.i, Y)))
    if (theta.j >1 | theta.j <0){alpha.ij <- 0}
    u <- runif(1)
    if( u < alpha.ij){theta.i <- theta.j}
    chain[k] <- theta.i
  }
  chain
}

func_MetroDyn <- function(startseed, m, Y, driver){
  #dynamic driver 
  set.seed(startseed)
  theta.i <- runif(1)
  chain <- c()
  alpha <- c()
  chain[1] <- theta.i
  for (k in 2:m){
    theta.j <- driver(1, theta.i)
    alpha.ij <- min(c(1, func_pi(theta.j, Y)/func_pi(theta.i, Y)))
    if (theta.j >1 | theta.j <0){alpha.ij <- 0}
    u <- runif(1)
    if( u < alpha.ij){theta.i <- theta.j}
    chain[k] <- theta.i
  }
  chain
}

func_driver1.Uniform <-    function(n){runif(n, min=0, max=1) }
func_driver2.Norm.sd.01 <- function(n, mu){rnorm(n, mean=mu, sd=0.01)}
func_driver3.Norm.sd.1 <-  function(n, mu){rnorm(n, mean=mu, sd=0.1)}
func_driver4.Norm.sd.5 <-  function(n, mu){rnorm(n, mean=mu, sd=0.5)}
func_driver5.Norm.mu.4sd.5 <-  function(n){rnorm(n, mean=0.4, sd=0.5)}
```


```{r p2.func_p2ABC}
func_problem2ABC <- function(chain, driver.name){
#main title
print.Y <- paste(Y, collapse=",")
main <- paste("Metropolis for data Y=(", print.Y, ") using the driver", driver.name)

df <- data.frame(
         "theta.i"=chain
        ,"i"=c(1:length(chain))
)


#table grob
chain.mu <- mean(chain)
chain.sd <- sd(chain)
table <- data.frame(
  "Name" = c("Mean", "SD", "it", "Start"),
  "Value" = c(
    decimal(chain.mu, dec), 
    decimal(chain.sd, dec), 
    decimal(length(chain), 0),
    decimal(chain[1], dec)
  )
)
tg <- tableGrob(table)

#plot theta_i versus i (iterations)
plot <- ggplot(df, aes(x=i, y=theta.i))+
  geom_line(alpha=0.4)

grid.arrange(plot, tg, widths=c(4,1)
             ,top = textGrob(main, vjust = .5, gp = gpar(fontface = "bold", cex = 1.1))
                             )
}
```

```{r p2aRESULT, fig.height=2}
Y.A <- c(125, 18, 20, 34)

func_problem2ABC(func_MetroFix(211, 10000, Y.A, func_driver1.Uniform), "Uniform(0,1)")
func_problem2ABC(func_MetroDyn(212, 10000, Y.A, func_driver2.Norm.sd.01),   "Normal(theta.i, 0.01)")
func_problem2ABC(func_MetroDyn(213, 10000, Y.A, func_driver3.Norm.sd.1 ),   "Normal(theta.i, 0.10)")
func_problem2ABC(func_MetroDyn(214, 10000, Y.A, func_driver4.Norm.sd.5 ),   "Normal(theta.i, 0.50)")
func_problem2ABC(func_MetroFix(215, 10000, Y.A, func_driver5.Norm.mu.4sd.5),"Normal(0.40, 0.10)")
```

\newpage  
## Problem 2b  
Repeat 2a for $Y = (14, 0, 1, 5)$  
```{r p2bRESULT, fig.height=2}
Y.B <- c(14, 0, 1, 5)

func_problem2ABC(func_MetroFix(221, 10000, Y.B, func_driver1.Uniform), "Uniform(0,1)")
func_problem2ABC(func_MetroDyn(222, 10000, Y.B, func_driver2.Norm.sd.01),   "Normal(theta.i, 0.01)")
func_problem2ABC(func_MetroDyn(223, 10000, Y.B, func_driver3.Norm.sd.1 ),   "Normal(theta.i, 0.10)")
func_problem2ABC(func_MetroDyn(224, 10000, Y.B, func_driver4.Norm.sd.5 ),   "Normal(theta.i, 0.50)")
func_problem2ABC(func_MetroFix(225, 10000, Y.B, func_driver5.Norm.mu.4sd.5),"Normal(0.40, 0.10)")
```

## Problem 2c  
Compute both the posterior mean and standard deviation for both data sets.  
Compare to results from the previous problem.  

In problem 1 the means were similar, but in problem 2 the means vary depending on the driver.  Some of the means in problem 2 are close to the means in problem 1.  

\newpage  
## Problem 2d  

```{r p2.func_Metroinfo}
func_Metroinfo <- function(startseed, m, Y, driver, metro.func){
chain <- metro.func(startseed, m, Y, driver)
it <- length(chain)
chain.mu <- mean(chain)
chain.sd <- sd(chain)
chain.se <- chain.sd/sqrt(it)
vec <-c(chain.mu, chain.sd, chain.se)
return(vec)
}
```

```{r p2.func_20chains}
func_20metrochains <- function(Y, driver, metro.func){
print.Y <- paste(Y, collapse=",")
cnames <- c("Mean", "SD", "SE")
info.20chain <- mapply( func_Metroinfo
                       , startseed=c(1:20)
                       , m =rep(10000, 20)
                       , Y=rep(list(Y), 20)
                       , driver=rep(list(driver), 20)
                       , metro.func=rep(list(metro.func), 20)
                      )
info.20chain <- t(info.20chain)
colnames(info.20chain) <- cnames
info.20chain
}
```

```{r p2dRESULT20chains}
Y.D <- c(125, 18, 20, 34)

d1 <- func_20metrochains(Y.D, func_driver1.Uniform,      func_MetroFix)
d2 <- func_20metrochains(Y.D, func_driver2.Norm.sd.01,    func_MetroDyn)
d3 <- func_20metrochains(Y.D, func_driver3.Norm.sd.1,     func_MetroDyn)
d4 <- func_20metrochains(Y.D, func_driver4.Norm.sd.5,     func_MetroDyn)
d5 <- func_20metrochains(Y.D, func_driver5.Norm.mu.4sd.5, func_MetroFix)

table2d <- cbind(d1, d2, d3, d4, d5)

print.Y.D <- paste(Y.D, collapse=",")

knitr::kable(table2d, digits=4, booktabs=TRUE, 'latex'
             , caption=paste("20 Chains for Y=(",print.Y.D, ") with Different Drivers")
             ) %>%
  kableExtra::kable_styling(latex_options=c("hold_position", "scale_down") ) %>% 
  kableExtra::add_header_above(c( "Uniform(0, 1)" =3
                                 ,"Normal(theta.i, 0.01)"=3
                                 ,"Normal(theta.i, 0.10)"=3
                                 ,"Normal(theta.i, 0.50"=3
                                 ,"Normal(0.40, 0.10)"=3
                                ))
```

```{r p2dRESULTavgsd.vs.se}
sd.of.avgs <- c(
   sd(table2d[,1])
  ,sd(table2d[,4])
  ,sd(table2d[,7])
  ,sd(table2d[,10])
  ,sd(table2d[,13])
  )

avg.of.se <- c(
   mean(table2d[,3])
  ,mean(table2d[,6])
  ,mean(table2d[,9])
  ,mean(table2d[,12])
  ,mean(table2d[,15])
  )

driver.name <- c(
  "Uniform(0,1)"
  ,"Normal(theta.i, 0.01)"
  ,"Normal(theta.i, 0.10)"
  ,"Normal(theta.i, 0.50)"
  ,"Normal(0.40, 0.10)"
)


table2d.sdse <- cbind(sd.of.avgs, avg.of.se)
rownames(table2d.sdse) <- driver.name

knitr::kable(table2d.sdse, 'latex', booktabs=TRUE
             , caption=paste("SD of Average theta's versus Average SE")
             )  %>%
    kableExtra::kable_styling(latex_options="hold_position")
```

Similar to problem 1, it appears that our estimation of the variation in the mean of theta is lower than the actual variation between the means.   
 

\newpage  
# Problem 3  

## Problem 3a  
Consider the 1-way variance components model
$$Y_{ij} = \theta_i + \epsilon_{ij}$$ 
where $Y_{ij}$ is the $j$th observation from the $i$th group, $\theta_i$ is the effect, $\epsilon_{ij}$=error,
$i=1, ..., K$ and $j=1, ..., J$.
It is assume that $\epsilon_{ij} \stackrel{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2_{\epsilon})$
and $\theta_i \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2_{\theta})$.
Under the prior specification $p(\sigma^2_{\epsilon}, \sigma^2_{\theta}, \mu) =$
$p(\sigma^2_{\epsilon})p(\sigma^2_{\theta})p(\mu)$, with
$p(\sigma_\theta^2) = \text{InverseGamma}(a_1, b_1)$, 
$p(\sigma_\epsilon^2) = \text{InverseGamma}(a_2, b_2)$, and
$p(\mu) = \mathcal{N}(\mu_0, \sigma^2_0)$.  
Let $\overline{Y}_i = \frac{1}{J} \sum_{j=1}^J Y_{ij}$ and $\theta = (theta_1, \cdots, \theta_k)$.  
Show the following: 

### Problem 3a(1) 
$$
p(\mu \mid \theta, \sigma^2_{\epsilon}, \sigma^2_{\theta}, Y) 
= \mathcal{N} \left( 
\frac{ \sigma_\theta^2 \mu_0 + \sigma_0^2 \sum \theta_i}{\sigma^2_\theta + K \sigma^2_0}
, \ 
\frac{ \sigma^2_\theta \sigma^2_0}{\sigma^2_\theta + K \sigma^2_0}
\right)
$$

### Problem 3a(2)  
$$
p(\theta_i \mid \mu, \sigma^2_\epsilon, \sigma^2_\theta, Y)
= \mathcal{N} \left( 
\frac{J\sigma^2_\theta}{J\sigma^2_\theta + \sigma_\epsilon^2} \cdot \overline{Y}_i
+ 
\frac{\sigma^2_\epsilon}{J\sigma^2_\theta + \sigma_\epsilon^2} \cdot \mu
, \ \ \ 
\frac{\sigma^2_\theta \sigma^2_\epsilon}{J\sigma^2_\theta + \sigma_\epsilon^2}
\right) 
$$

### Problem 3a(3)  
$$
p(\sigma^2_\epsilon \mid \mu, \theta, \sigma^2_\theta, Y)  = \text{InverseGamma}
\left(
a_2 + \frac{KJ}{2}, \ 
b_2 + \frac{1}{2} \sum_{i=1}^K \sum_{j=1}^J (Y_{ij} - \theta_i)^2
\right)
$$

### Problem 3a(4) 
$$
p(\sigma^2_\theta \mid \mu, \theta, \sigma^2_\epsilon, Y)  = \text{InverseGamma}
\left(
a_1 + \frac{K}{2}
, \ 
b_1 + \frac{1}{2} \sum_{i=1}^K  (\theta_i - \mu)^2
\right)
$$

## Problem 3b  
Run the Gibbs sampler for the data below. 
Use one chain of length 75,000.
Take $p(\mu) =\mathcal{N}(0, 10^{12})$,  $p(\sigma^2_\epsilon) = IG(0, 0)$, and $p(\sigma^2_\theta) = IG(1, 1)$.
For each $\theta_i$, for $\sigma_\epsilon$, and for $\theta_\theta$, plot the simulated value at iteration $j$ versus $j$.
Summarize each posterior marginal.  

## Problem 3c 
Repeat 3b using the prior specification $p(\mu) = \mathcal{N}(0, 10^{12})$, $p(\sigma_\epsilon^2) = IG(0, 0)$, and
$p(\sigma^2_\theta) = IG(0, 0)$.  Does this specification violate the Hobart_Casella conditions?
Describe what happens to the Gibbs sampler chain in this case.  

# Problem 5  
Suppose that $X$ and $Y$ have exponential conditional distributions restricted over the interval $(0, B)$, i.e. 
$p(x \mid y) \propto y \exp \left\{ -yx \right\}$ for $0 < x < B < \infty$ and 
$p(y \mid x) \propto x \exp \left\{ -xy \right\}$ for $0 < y < B < \infty$, where $B$ is known constant.  

## Problem 5a  
Take $m=1$ and $B=3$.  Run the data augmentation algorithm using these conditionals.
(Hist: Reject the exponential deviates that lie outside $(0, B)$).
How did you assess convergence of this chain? 
Obtain the marginal for $x$ using the mixture of conditionals $p(x \mid y)$, mixed over the simulated $y$ deviates in your chain.  

## Problem 5b  
Show that the marginal for $x$ is proportional to $(1 - \exp\left\{ -Bx \right\})/x$.
Compare your results in 5a to this curve.  
 
## Problem 5c 
Repeat 5a and 5b using $B= \infty$.  Describe what happens.  Is the marginal for $x$ a proper density in this case?  