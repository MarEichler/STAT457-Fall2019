---
title: STAT 457 Homework 05
author: Martha Eichlersmith
date: 2019-11-16
output:
  pdf_document:
    fig_caption: yes
header-includes:
  - \usepackage{color}
  - \usepackage{mathtools}
  - \usepackage{amsbsy} #bold in mathmode
  - \usepackage{nicefrac} # for nice fracs 
  - \usepackage{booktabs}
  - \usepackage{geometry}
  - \usepackage{caption} #to remove automatic table name and number - \captionsetup[table]{labelformat=empty}, put code under ---
geometry: "left=1.75cm,right=1.75cm,top=1.5cm,bottom=2cm" 

---

\captionsetup[table]{labelformat=empty} 
```{r setup, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
library(ggplot2) #ggplot
library(readr) #import CSV
library(gridExtra) #organize plots
library(grid) #organize plots
library(latex2exp) #latex in ggplot titles 
library(matlib) #A = matrix, inv(A) = A^{-1} 
library(numDeriv) #calculate numerical first and second order derivatives 
library(gtable) #for tablegrob functions 
#library(kableExtra) #for kable functions
library(dplyr) #for piping 
knitr::opts_chunk$set(echo=FALSE, fig.width = 10, fig.height = 4)
decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
dec <- 5
#knitr::opts_chunk$set(echo=FALSE) #using knitr for this option but don't have to load 
```

## Problem 1  
Consider two urns each containing an unknown mixture of blue and white marbles.  A random sample of size 18 (with replacement) is drawn from urn #1 and a random sample of size 6 (with replacement) is drawn from urn #2.  Of the 18 selected marbles from urn #1, 14 are blue.  The corresponding number of blue marbles from urn #2 is 2.  

$$
\begin{aligned}
L(\pi | Y) & \propto \pi^{y} (1 - \pi)^{n - y} = \pi^{14}(1 - \pi)^{4} \sim \text{Binomial}, n_{\pi} = 18, \ y_{\pi} = 14
\\[0.5ex]
& \implies  p(\pi \mid Y)  \sim \text{Beta}(y + \alpha_0, n - y + \beta_0) \quad \text{where} \quad p(\pi) \sim \text{Beta}(\alpha_0, \beta_0) 
\\[2.5ex]
L(\psi | Y) & \propto \psi^{y} (1 - \psi)^{n - y} = \psi^{2}(1 - \psi)^{4} \sim \text{Binomial}, n_{\pi} = 6, \ y_{\pi} = 2
\\[0.5ex]
& \implies  p(\psi \mid Y)  \sim \text{Beta}(y + \alpha_0, n - y + \beta_0) \quad \text{where} \quad p(\psi) \sim \text{Beta}(\alpha_0, \beta_0) 
\end{aligned}
$$

### Problem 1a  
Let $\pi$ denote the proportion of blue marbles in urn #1 and let $\psi$ denote the corresponding proportion in urn #2.  Under the (i) Haldane, (ii) flat and (iii) non-informative priors, compute $p \left( \ln \left[  \frac{ \pi}{1 - \pi} \right]  > \ln \left[  \frac{ \psi}{1 - \psi} \right] \mid \text{data} \right)$ using the normal approximation.  

$$
p \left( \ln \left[  \frac{ \pi}{1 - \pi} \right]  > \ln \left[  \frac{ \psi}{1 - \psi} \right] \mid \text{data} \right) 
= 
p \left( \ln \left[  \frac{ \pi}{1 - \pi} \right]  - \ln \left[  \frac{ \psi}{1 - \psi} \right] > 0  \mid \text{data} \right) 
$$
```{r p1a, warning=FALSE}
n.pi <- 18
n.psi <- 6 
y.pi <- 14
y.psi <- 2
pi <- y.pi / n.pi
psi <- y.psi / n.psi

func_approxNormpval <- function(a0, b0){
alpha <- y.pi + a0
gamma <- y.psi + a0
beta  <- n.pi - y.pi + b0
delta <- n.psi - y.psi + b0
post.pi <- (y.pi+a0 )/ n.pi
post.psi <- (y.psi+a0) / n.psi
inside.log <- (post.pi / (1 - post.pi)) / (post.psi/(1 - post.psi))
approxNorm.mean <- log( inside.log )
approxNorm.var <- (1/alpha) + (1/beta) + (1/gamma) + (1/delta)
pval <- 1-pnorm(0, approxNorm.mean, approxNorm.var)
pval 
}

names <- c("Beta(0, 0)","Beta(1, 1)", "Beta(.5, .5)")
pvals <- c(func_approxNormpval(0, 0), func_approxNormpval(1,1), func_approxNormpval(.5, .5))
results <- rbind(names, decimal(pvals, dec))
rownames(results) <- c("Prior", "p-value")

knitr::kable(results, booktabs=TRUE, 'latex', caption="Normal Approx: Probability difference in logodds is greater than 0") %>%
  kableExtra::kable_styling(latex_options="hold_position" ) %>% #hold table in place 
  kableExtra::add_header_above(c(" "=1, "Haldane"=1, "Flat"=1, "Non-informative"=1)) #need to have a space in empty columns 
```


### Problem 1b  
Repeat (1a) by drawing deviates from the appropriate beta distributions.  Quantify the Monte Carlo error in your value.   
```{r p1b, warning=FALSE}
set.seed(050101)

n.pi <- 18
n.psi <- 6 
y.pi <- 14
y.psi <- 2
 
func_pval <- function(it, a0, b0){
a0 <- 0 
b0 <- 0
a.pi <- n.pi + a0
b.pi <- n.pi - y.pi + b0
a.psi <- n.psi + a0
b.psi <- n.psi - y.psi + b0

post.pi <- rbeta(it, a.pi, b.pi)
post.psi <- rbeta(it, a.psi, b.psi)

difflogodds <- log(post.pi/(1 - post.pi)) - log(post.psi/(1 - post.psi)) 
se <- sd(difflogodds)/sqrt(it)
pval <- 1 - pnorm(0, mean(difflogodds), sd(difflogodds))
results <- c(pval, se)
}

func_logdiff <- function(it.vec, a0, b0){
a0.vec <- rep(a0, length(it.vec))
b0.vec <- rep(b0, length(it.vec))
pvals <- mapply(func_pval, it.vec, a0.vec, b0.vec)
table <- rbind(it.vec, decimal(pvals, dec))
rownames(table) <- c("Iterations", "p-value", "Standard Error")
table 
}

it.vec <- c(1e+04, 1e+05, 1e+06)

dt <- cbind(func_logdiff(it.vec, 0, 0), func_logdiff(it.vec, 1, 1), func_logdiff(it.vec,.5, .5))

knitr::kable(dt, booktabs=T, 'latex', caption="Probability that the log differences are greater than 0") %>%
  kableExtra::kable_styling(latex_options="hold_position" ) %>% #hold table in place 
  kableExtra::add_header_above(c(" "=1, "Haldane Beta(0, 0)"=3, "Flat Beta(1, 1)"=3, "Non-informative Beta(.5, .5)"=3)) #need to have a space in empty columns 
```

\newpage  
### Problem 1c 
Compare your results in (1a) and (1b) to the p-value obtained via Fisher's exact test.  
```{r p1c}
blue <- matrix(c(14, 2, 4, 4), nrow=2)
fisher.test(blue, or=1, alternative="less")
```

The Fisher Exact test p-value is larger than the normal approximation or the Monte Carlo methods.  

### Problem 1d  
Add delinquency problem 

## Problem 2  
Suppose a sample of size $n$ is drawn at random and with replacement from some population.  For large $n$ the sample proportion $(\hat{p})$ is normally distributed with mean $p$ and variance $\frac{ p(1 - p)}{n}$.  Find the asymptotic distribution of $2\sin^{-1}\sqrt{\hat{p}}$ using the delta method.  

## Problem 3  
Let $x_1, \cdots, x_n$ be an iid sample from $\mathcal{N}(\theta, 1)$ and let $y_1, \cdots , y_n$ be an independent iid sample from $\mathcal{N}(\phi, 1)$.  Derive the distribution of $\overline{x} / \overline{y}$ (where $\overline{y} \neq 0$) via the delta method. 

## Problem 4  
197 animals are distributed into four categories: $Y = (y_1, y_2, y_3, y_4)$ according to the genetic linkage model $\left( \frac{2 + \theta}{4}, \frac{1 - \theta}{4}, \frac{1- \theta}{4}, \frac{ \theta}{4} \right)$.  In HW#4 you derived the likelihood for the data $Y = (125, 18, 20, 34)$ and you derived the likelihood for the data $Y = (14, 0, 1, 15)$.  In that homework, you also used Newton-Raphson algorithm to obtain the MLE ($\hat{\theta}$) of $\theta$ and the standard error of $\hat{\theta}$.  
```{r p4-graph}
func_normlike<-function(x,y1,y2,y3,y4){
  like <-(2+x)^y1*(1-x)^(y2+y3)*(x)^y4
  like.max<-max(like)
  like/like.max #normalized likelihood (on scale from 0 to 1) 
}

func_scalenormal<-function(x, mean, sd){
  scales::rescale(dnorm(x, mean, sd), to=c(0, 1)) #normal (on scale from 0 to 1) 
}

func_plots <- function(yval, theta.mle, se){
colors <- c("navy", "maroon")
norm.like <-    stat_function(fun = func_normlike, args = list(y1=yval[1], y2=yval[2], y3=yval[3], y4=yval[4]), lwd = 1.5, linetype="solid", aes(col="Normalized Likelihood"))
normal.approx <-stat_function(fun = func_scalenormal, args = list(mean=theta.mle, sd=se), lwd = 2.5, linetype="dotted", aes(col="Normal Approximation"))

print.yval <- paste(yval, collapse=", ")
name <- paste("Normal Likelihood and Normal Approximation for Y=(", print.yval, ")")

table <- data.frame("Statistic"=c("MLE", "Standard Error")
                    , "Value"=decimal(c(theta.mle, se), dec))
tg <- tableGrob(table)

x <- seq(0, 1, 0.001)
df <- data.frame("X"=x)
ggplot(data=df, aes(x=X))+
  norm.like+normal.approx+
  ggtitle(paste(name))+
  theme(axis.title.x = element_blank())+
  scale_colour_manual("", values = c(colors[1], colors[2])) +
  theme(legend.position = "bottom")+
  annotation_custom(tg, xmin=.25, ymin=.25, xmax=0.5, ymax=1)

}
```

```{r p4-netwonraphson}
func_newton.raphson <- function(f, start, it, tol){
  x0 <- start
  k <- c()
  for (i in 1:it) {
    x1 <- x0 - f(x0) #calcualte next value x1
    k[i] <- x1 #store x1 
    root.approx <- tail(k, n=1)
    it.completed <- length(k) 
    # Once the difference between x0 and x1 becomes sufficiently small, output the results.
    if (abs(x1 - x0) < tol & !is.na(abs(x1- x0)))
      { root.approx 
      break} 
    else if( it.completed == it){print(paste("Start at", start, ": diverges"))} 
    else{ x0 <- x1}
  }
  return(root.approx)
}
tol <- 1e-5
it <- 1000

func_se <- function(theta.mle){
  secondderiv <-  -(yval[1] / (2 + theta.mle)^2)  - ((yval[2] + yval[3])/(1 - theta.mle)^2 )- (yval[4]/theta.mle)
  se <- sqrt(1/(-secondderiv))
  se #standard error is 1 /Information evaluate at MLE, Information = -2nd deriv of log like  
}
```


### Problem 4a  
Plot the normalized likelihood and the associated normal approximation in the same figure for the data $Y= (125, 18, 20, 34)$.  Discuss the adequacy of the normal approximation. 

```{r p4a, warning=FALSE}
yval <- c(125, 18, 20, 34)
func_a<-function(x){
 (
  yval[1]/(2+x)-(yval[2]+yval[3])/(1-x)+yval[4]/x
  ) / (
  -yval[1]/(2+x)^2-(yval[2]+yval[3])/(1-x)^2-yval[4]/x^2
  ) #iterations of newton raphson = theta^{i + 1} = theta^{i} - (1st deriv of log like) / (2nd deriv of log like)
}

theta.mle <- func_newton.raphson(func_a, 0.6, it, tol)
se <- func_se(theta.mle)

func_plots(yval, theta.mle, se)
```

### Problem 4b  
Repeat (4a) for $Y = (14, 0, 1, 5)$  
```{r p4b, warning=FALSE}
yval <- c(14, 0, 1, 5)
func_b<-function(x){
 (
  yval[1]/(2+x)-(yval[2]+yval[3])/(1-x)+yval[4]/x
  ) / (
  -yval[1]/(2+x)^2-(yval[2]+yval[3])/(1-x)^2-yval[4]/x^2
  ) #iterations of newton raphson = theta^{i + 1} = theta^{i} - (1st deriv of log like) / (2nd deriv of log like)
}

theta.mle <- func_newton.raphson(func_b, 0.9, it, tol)
se <- func_se(theta.mle)

func_plots(yval, theta.mle, se)
```

## Problem 5  
Use Laplace's method (second order) to compute the posterior mean (under a flat prior) for the genetic linkage model for both data sets.  
